{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "uZRc9yXFhveu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-I6zlS6qkZGB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "! yes y | pip uninstall torchtext\n",
        "! yes y | pip uninstall torchaudio\n",
        "! yes y | pip uninstall flask\n",
        "! yes y | pip uninstall en-core-web-sm\n",
        "! pip install pattern\n",
        "! pip install -U pip setuptools wheel\n",
        "! pip install -U spacy==3.1.0\n",
        "! python -m spacy download en_core_web_sm\n",
        "! pip install allennlp allennlp-models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eYJg64MEmYIp",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.tagging\n",
        "from copy import deepcopy\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.tree import Tree\n",
        "from nltk.corpus import conll2000, wordnet\n",
        "import re\n",
        "from pattern.en import conjugate, lemma, lexeme, PRESENT, SG, PAST, PL\n",
        "import random\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import inflect\n",
        "inflect = inflect.engine()\n",
        "\n",
        "# Load the language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "wl = WordNetLemmatizer()\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('conll2000')\n",
        "\n",
        "SRL_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/structured-prediction-srl-bert.2020.12.15.tar.gz\")\n",
        "CR_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\")\n",
        "NER_predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/ner-elmo.2021-02-12.tar.gz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rGO8E78guaX"
      },
      "source": [
        "# Helping Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def import_error():\n",
        "  from pattern.en import conjugate, lemma, lexeme, PRESENT, SG, PAST, PL"
      ],
      "metadata": {
        "id": "IXITlc8g-HDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "A74CNB8E4Ts4"
      },
      "outputs": [],
      "source": [
        "def find_cluster(cluster, words):\n",
        "  all_coref = []\n",
        "  for coref in cluster:\n",
        "    coref_ =  ' '.join(words[coref[0]: coref[1] + 1])\n",
        "    print(coref_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_vynhuSLMBgG"
      },
      "outputs": [],
      "source": [
        "def find_arg(words, tags, tag_id):\n",
        "  founds = []\n",
        "  found_start = False\n",
        "  for index, (word, tag) in enumerate(zip(words, tags)):\n",
        "    if tag_id in tag and not 'R-ARG' in tag:\n",
        "      found_start = True\n",
        "      founds.append([word, tag, index])\n",
        "  real_founds = ' '.join([found[0] for found in founds])\n",
        "  if len(founds) != 0:\n",
        "    indexes = [founds[0][2], founds[-1][2]]\n",
        "  else:\n",
        "    indexes = []\n",
        "  case_exists = True if real_founds != '' else False\n",
        "  return real_founds, case_exists, indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LDBYbmIGBq7m"
      },
      "outputs": [],
      "source": [
        "def find_sents_bounds(string):\n",
        "  l_token = -1\n",
        "  token_bounds = []\n",
        "\n",
        "  for sent in sent_tokenize(string):\n",
        "    c_token = string.find(sent)\n",
        "    if c_token > l_token:\n",
        "      token_bounds.append([c_token, c_token + len(sent)])\n",
        "    else:\n",
        "      c_token = string[l_token: ].find(sent) + l_token\n",
        "\n",
        "    if c_token < l_token:\n",
        "      assert False\n",
        "\n",
        "    if c_token == -1:\n",
        "      assert False\n",
        "    l_token = c_token + len(sent)\n",
        "  return token_bounds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0M8XFzH9YJaO"
      },
      "outputs": [],
      "source": [
        "def find_in_sent(doc, span_bound, string, mode='start'):\n",
        "  index = span_bound\n",
        "  to_find = doc[index]\n",
        "  to_find_len = len(to_find)\n",
        "  finds = list(re.finditer(re.escape(to_find), string))\n",
        "  finds_1 = finds\n",
        "  finds_2 = finds\n",
        "  if len(finds) == 1:\n",
        "    span = finds[0].span()\n",
        "  else:\n",
        "    while True:\n",
        "      if index + 1 < len(doc):\n",
        "        index += 1\n",
        "        to_find_1 = to_find + doc[index]\n",
        "        to_find_2 = to_find + ' ' + doc[index]\n",
        "\n",
        "        finds_1 = list(re.finditer(re.escape(to_find_1), string))\n",
        "        finds_2 = list(re.finditer(re.escape(to_find_2), string))\n",
        "\n",
        "        if len(finds_1) == 1:\n",
        "          break\n",
        "        elif len(finds_1) > 1:\n",
        "          to_find = to_find + doc[index]\n",
        "          continue\n",
        "        elif len(finds_2) == 1:\n",
        "          break\n",
        "        elif len(finds_2) > 1:\n",
        "          to_find = to_find + ' ' + doc[index]\n",
        "          continue\n",
        "\n",
        "  if len(finds_1) == 1:\n",
        "    A = finds_1[0]\n",
        "  elif len(finds_2) == 1:\n",
        "    A = finds_2[0]\n",
        "\n",
        "  if mode == 'start':\n",
        "    return A.span()[0]\n",
        "  elif mode == 'end':\n",
        "    return A.span()[0] + to_find_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NriJeWL54Uku"
      },
      "outputs": [],
      "source": [
        "def find(doc, span_bound, mode='start'):\n",
        "  index = span_bound\n",
        "  to_find = doc[index]\n",
        "  to_find_len = len(to_find)\n",
        "  finds = list(re.finditer(re.escape(to_find), string))\n",
        "  finds_1 = finds\n",
        "  finds_2 = finds\n",
        "  if len(finds) == 1:\n",
        "    span = finds[0].span()\n",
        "  else:\n",
        "    while True:\n",
        "      if index + 1 < len(doc):\n",
        "        index += 1\n",
        "        to_find_1 = to_find + doc[index]\n",
        "        to_find_2 = to_find + ' ' + doc[index]\n",
        "        finds_1 = list(re.finditer(re.escape(to_find_1), string))\n",
        "        finds_2 = list(re.finditer(re.escape(to_find_2), string))\n",
        "\n",
        "        if len(finds_1) == 1:\n",
        "          break\n",
        "        elif len(finds_1) > 1:\n",
        "          to_find = to_find + doc[index]\n",
        "          continue\n",
        "        elif len(finds_2) == 1:\n",
        "          break\n",
        "        elif len(finds_2) > 1:\n",
        "          to_find = to_find + ' ' + doc[index]\n",
        "          continue\n",
        "      else:\n",
        "        break\n",
        "\n",
        "  if len(finds_1) == 1:\n",
        "    A = finds_1[0]\n",
        "  elif len(finds_2) == 1:\n",
        "    A = finds_2[0]\n",
        "  elif len(finds_1) > 1:\n",
        "    A = finds_1[-1]\n",
        "  elif len(finds_2) > 1:\n",
        "    A = finds_2[-1]\n",
        "\n",
        "  if mode == 'start':\n",
        "    return A.span()[0]\n",
        "  elif mode == 'end':\n",
        "    return A.span()[0] + to_find_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "txyCFstODyX_"
      },
      "outputs": [],
      "source": [
        "def find_sent(bound):\n",
        "  for sent_idx, sent_bound in enumerate(sents_bounds):\n",
        "    if bound[0] >= sent_bound[0] and bound[1] <= sent_bound[1]:\n",
        "      return sent_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jUskFNo_Q6KH"
      },
      "outputs": [],
      "source": [
        "def not_possessive(coref):\n",
        "  possessives = ['mine', 'yours', 'his', 'her', 'ours', 'theirs', '\\'s']\n",
        "  for possessive in possessives:\n",
        "    if possessive in coref.lower():\n",
        "      return False\n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "MNLgEzjZSs_4"
      },
      "outputs": [],
      "source": [
        "def find_pronoun(coref):\n",
        "  coref = coref.lower()\n",
        "  if coref in ['i', 'you', 'he', 'she', 'we', 'they']:\n",
        "    return coref\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ci1XOgB4W5iv"
      },
      "outputs": [],
      "source": [
        "def find_corresponding_word(char_bound, sent_mapping, mode='start'):\n",
        "  for k, v in sent_mapping.items():\n",
        "    if mode == 'start':\n",
        "      if v[0] == char_bound[0]:\n",
        "        return k\n",
        "    elif mode == 'end':\n",
        "      if v[1] == char_bound[1]:\n",
        "        return k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_QuIDQ3RjN7j"
      },
      "outputs": [],
      "source": [
        "def print_ner(sent):\n",
        "  ner = NER_predictor.predict(sent)\n",
        "  for word, tag in zip(ner['words'], ner['tags']):\n",
        "    print(word, tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RR8KAwvpn8MB"
      },
      "outputs": [],
      "source": [
        "def what_is_coref(all_indexes, coref_bounds):\n",
        "  if all_indexes[0] != [] and coref_bounds[0] == all_indexes[0][0] and coref_bounds[1] == all_indexes[0][1]:\n",
        "    print('coref is extent')\n",
        "  elif all_indexes[1] != [] and coref_bounds[0] == all_indexes[1][0] and coref_bounds[1] == all_indexes[1][1]:\n",
        "    print('coref is time')\n",
        "  elif all_indexes[2] != [] and coref_bounds[0] == all_indexes[2][0] and coref_bounds[1] == all_indexes[2][1]:\n",
        "    print('coref is manner')\n",
        "  elif all_indexes[3] != [] and coref_bounds[0] == all_indexes[3][0] and coref_bounds[1] == all_indexes[3][1]:\n",
        "    print('coref is agent')\n",
        "  elif all_indexes[4] != [] and coref_bounds[0] == all_indexes[4][0] and coref_bounds[1] == all_indexes[4][1]:\n",
        "    print('coref is patient')\n",
        "  elif all_indexes[5] != [] and coref_bounds[0] == all_indexes[5][0] and coref_bounds[1] == all_indexes[5][1]:\n",
        "    print('coref is state')\n",
        "  elif all_indexes[6] != [] and coref_bounds[0] == all_indexes[6][0] and coref_bounds[1] == all_indexes[6][1]:\n",
        "    print('coref is cause')\n",
        "  elif all_indexes[7] != [] and coref_bounds[0] == all_indexes[7][0] and coref_bounds[1] == all_indexes[7][1]:\n",
        "    print('coref is verb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "B7XmM-7f1XNr"
      },
      "outputs": [],
      "source": [
        "def get_sent(*args):\n",
        "  sent = ''\n",
        "  for x in args:\n",
        "    sent += x + ' '\n",
        "  sent = re.sub('\\s\\s+', ' ', sent)\n",
        "  return sent.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "xck0rQX2RqaD"
      },
      "outputs": [],
      "source": [
        "def extract_entities(sent):\n",
        "  tags = NER_predictor.predict(sent)['tags']\n",
        "  named_entities = []\n",
        "  start = 0\n",
        "  end = 0\n",
        "  for tag_id, tag in enumerate(tags):\n",
        "\n",
        "    if tag.startswith('U'):\n",
        "      start, end = tag_id, tag_id\n",
        "      entity_type = tag.split('-')[-1]\n",
        "      named_entities.append([entity_type, start, end])\n",
        "\n",
        "    elif tag.startswith('B'):\n",
        "      found_start = True\n",
        "      start = tag_id\n",
        "\n",
        "    elif tag.startswith('L'):\n",
        "      end = tag_id\n",
        "      entity_type = tag.split('-')[-1]\n",
        "      named_entities.append([entity_type, start, end])\n",
        "\n",
        "  return named_entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "P7lpfrJUTPbW"
      },
      "outputs": [],
      "source": [
        "def is_person(entities):\n",
        "  if 'PER' in entities:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "euTyJd7aAakA"
      },
      "outputs": [],
      "source": [
        "def find_semantic_ent_type(entities, semantic_roles_indexes):\n",
        "  semantic_roles_ent_types = [[] for i in range(8)]\n",
        "  for entity in entities:\n",
        "    for idx, semantic_roles_index in enumerate(semantic_roles_indexes):\n",
        "      if semantic_roles_index != []:\n",
        "        if semantic_roles_index[0] <= entity[1] and semantic_roles_index[1] >= entity[2]:\n",
        "          semantic_roles_ent_types[idx].append(entity[0])\n",
        "  return semantic_roles_ent_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DBIjRbX2Vl-J"
      },
      "outputs": [],
      "source": [
        "def find_num_ent(entities, semantic_roles_index):\n",
        "  num_ent = 0\n",
        "  for entity in entities:\n",
        "    if semantic_roles_index[0] <= entity[1] and semantic_roles_index[1] >= entity[2]:\n",
        "      num_ent += 1\n",
        "  return num_ent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GZksJVOlxUV-"
      },
      "outputs": [],
      "source": [
        "def find_corresponding_coref(indexes, sent_idx):\n",
        "  all_names, all_indexes = list(), [None]\n",
        "  for cluster_idx, cluster in enumerate(chars_spans):\n",
        "    for coref in cluster:\n",
        "      if indexes != []:\n",
        "        if coref[1] == sent_idx and coref[2] == indexes[0] and coref[3] == indexes[1]:\n",
        "          if cluster_idx not in all_indexes:\n",
        "            if all_indexes[0] is None:\n",
        "              all_indexes = []\n",
        "            all_names.append(cluster_names[cluster_idx])\n",
        "            all_indexes.append(cluster_idx)\n",
        "  return all_names, all_indexes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yW-oj0r3_v7d"
      },
      "outputs": [],
      "source": [
        "def simple_verb(verb):\n",
        "  verbs = ['am', 'is', 'are', 'was', 'were']\n",
        "  if verb in verbs:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NUP2y57m_06e"
      },
      "outputs": [],
      "source": [
        "def non_trivial_corefs(corefs):\n",
        "  main_pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'its']\n",
        "  for coref in corefs:\n",
        "    if coref.lower() not in main_pronouns:\n",
        "      return coref\n",
        "  return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "V-XIVYQA_7Vh"
      },
      "outputs": [],
      "source": [
        "class Node:\n",
        "\n",
        "  def __init__(self, sent_num, coref_clusters):\n",
        "    self.sent_num = sent_num\n",
        "    self.coref_clusters = coref_clusters\n",
        "    self.edges = list()\n",
        "\n",
        "  def set_edge(self, adj_sent, edge_coref_type):\n",
        "    self.edges.append([adj_sent, edge_coref_type])\n",
        "\n",
        "  def get_edges(self):\n",
        "    return self.edges\n",
        "\n",
        "  def get_clusters(self):\n",
        "    return self.coref_clusters\n",
        "\n",
        "  def make_cluster_2_sent(self):\n",
        "    self.cluster_2_edge = dict()\n",
        "    for edge in self.edges:\n",
        "      if edge[1] not in self.cluster_2_edge.keys():\n",
        "        self.cluster_2_edge[edge[1]] = []\n",
        "      self.cluster_2_edge[edge[1]].append(edge[0])\n",
        "\n",
        "\n",
        "  def __str__(self):\n",
        "    print(self.sent_num, self.coref_clusters)\n",
        "    return ''\n",
        "\n",
        "  def __repr__(self):\n",
        "    print(self.sent_num, self.coref_clusters)\n",
        "    return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2C_DgdxxnOdt"
      },
      "outputs": [],
      "source": [
        "def is_simple_verb(verb):\n",
        "  if verb in ['am', 'is', 'are', 'was', 'were']:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "4wAte1W5nhnx"
      },
      "outputs": [],
      "source": [
        "def find_in_sent(doc, span_bound, string, mode='start'):\n",
        "  index = span_bound\n",
        "  to_find = doc[index]\n",
        "  to_find_len = len(to_find)\n",
        "  finds = list(re.finditer(re.escape(to_find), string))\n",
        "  finds_1 = finds\n",
        "  finds_2 = finds\n",
        "  if len(finds) == 1:\n",
        "    span = finds[0].span()\n",
        "  else:\n",
        "    while True:\n",
        "      if index + 1 < len(doc):\n",
        "        index += 1\n",
        "        to_find_1 = to_find + doc[index]\n",
        "        to_find_2 = to_find + ' ' + doc[index]\n",
        "\n",
        "        finds_1 = list(re.finditer(re.escape(to_find_1), string))\n",
        "        finds_2 = list(re.finditer(re.escape(to_find_2), string))\n",
        "\n",
        "        if len(finds_1) == 1:\n",
        "          break\n",
        "        elif len(finds_1) > 1:\n",
        "          to_find = to_find + doc[index]\n",
        "          continue\n",
        "        elif len(finds_2) == 1:\n",
        "          break\n",
        "        elif len(finds_2) > 1:\n",
        "          to_find = to_find + ' ' + doc[index]\n",
        "          continue\n",
        "\n",
        "  if len(finds_1) == 1:\n",
        "    A = finds_1[0]\n",
        "  elif len(finds_2) == 1:\n",
        "    A = finds_2[0]\n",
        "\n",
        "  if mode == 'start':\n",
        "    return A.span()[0]\n",
        "  elif mode == 'end':\n",
        "    return A.span()[0] + to_find_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "gcftEEG7j-WC"
      },
      "outputs": [],
      "source": [
        "def find_sents_bounds(string):\n",
        "  l_token = -1\n",
        "  token_bounds = []\n",
        "  sents = sent_tokenize(string)\n",
        "  for sent in sents:\n",
        "    c_token = string.find(sent)\n",
        "    if c_token > l_token:\n",
        "      token_bounds.append([c_token, c_token + len(sent)])\n",
        "    else:\n",
        "      c_token = string[l_token: ].find(sent) + l_token\n",
        "      token_bounds.append([c_token, c_token + len(sent)])\n",
        "\n",
        "    if c_token < l_token:\n",
        "      assert False\n",
        "    if c_token == -1:\n",
        "      assert False\n",
        "\n",
        "\n",
        "    l_token = c_token + len(sent)\n",
        "  return token_bounds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1iMhHYz72yL"
      },
      "source": [
        "# Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9iIG22a6xPT",
        "outputId": "3b2758cf-4397-4fcd-f088-8c41f888b9ae",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/allennlp/modules/token_embedders/pretrained_transformer_embedder.py:385: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Big Bang Theory\n",
            "its\n",
            "The Big Bang Theory\n",
            "The Big Bang Theory\n",
            "the show 's\n",
            "--------------------------------\n",
            "Jim Parsons\n",
            "Parsons\n",
            "his\n",
            "Parsons\n",
            "Parsons\n",
            "his\n",
            "Jim Parsons\n",
            "he\n",
            "he\n",
            "he\n",
            "--------------------------------\n",
            "its spinoff series Young Sheldon\n",
            "the latter series '\n",
            "Young Sheldon\n",
            "Young Sheldon\n",
            "--------------------------------\n",
            "Sheldon Lee Cooper , Ph.D. , Sc . D. ,\n",
            "The character 's\n",
            "he\n",
            "his\n",
            "his\n",
            "The adult Sheldon\n",
            "his\n",
            "Sheldon\n",
            "his\n",
            "He\n",
            "his\n",
            "Sheldon\n",
            "he\n",
            "he\n",
            "he\n",
            "he himself\n",
            "himself\n",
            "He\n",
            "him\n",
            "him\n",
            "Sheldon 's\n",
            "Sheldon 's\n",
            "Sheldon 's character\n",
            "Sheldon\n",
            "The character of Sheldon Cooper\n",
            "He\n",
            "his\n",
            "the role\n",
            "the role\n",
            "Sheldon\n",
            "his\n",
            "His\n",
            "he\n",
            "Sheldon\n",
            "his\n",
            "Young Sheldon\n",
            "his\n",
            "his intellectually gifted son\n",
            "his\n",
            "his\n",
            "his\n",
            "he\n",
            "Sheldon\n",
            "Sheldon\n",
            "his\n",
            "Sheldon 's\n",
            "his\n",
            "he\n",
            "him\n",
            "His\n",
            "his\n",
            "him\n",
            "his\n",
            "he\n",
            "Young Sheldon\n",
            "his\n",
            "him\n",
            "--------------------------------\n",
            "his colleague and best friend , Leonard Hofstadter ( Johnny Galecki )\n",
            "Leonard 's\n",
            "Leonard 's\n",
            "Leonard\n",
            "--------------------------------\n",
            "irony and sarcasm\n",
            "them\n",
            "--------------------------------\n",
            "highly idiosyncratic behavior and a general lack of humility , empathy , and toleration\n",
            "These characteristics\n",
            "--------------------------------\n",
            "autism spectrum disorder ( or what used to be classified as Asperger 's Syndrome )\n",
            "Asperger 's\n",
            "Asperger 's syndrome\n",
            "--------------------------------\n",
            "Johnny Galecki\n",
            "Johnny Galecki\n",
            "Galecki\n",
            "he\n",
            "--------------------------------\n",
            "Chuck Lorre\n",
            "Lorre\n",
            "--------------------------------\n",
            "Sheldon and his fraternal twin sister , Missy ,\n",
            "their\n",
            "their\n",
            "their\n",
            "--------------------------------\n",
            "His first word\n",
            "this\n",
            "--------------------------------\n",
            "their father , George Cooper Sr . , a football coach\n",
            "his father\n",
            "he\n",
            "George\n",
            "his father\n",
            "his\n",
            "he\n",
            "He\n",
            "his\n",
            "--------------------------------\n",
            "Galveston , Texas\n",
            "Galveston\n",
            "--------------------------------\n",
            "other coaches\n",
            "their\n",
            "--------------------------------\n",
            "Medford , a fictional small town in East Texas that is a three - hour drive from Dallas\n",
            "Medford\n",
            "--------------------------------\n",
            "the family\n",
            "his family\n",
            "--------------------------------\n",
            "\" Pop - Pop \" ,\n",
            "Pop - Pop 's\n",
            "Pop - Pop\n",
            "--------------------------------\n",
            "Christmas\n",
            "Christmas\n",
            "--------------------------------\n",
            "his maternal grandmother whom he affectionately calls \" Meemaw \"\n",
            "His aunt\n",
            "--------------------------------\n"
          ]
        }
      ],
      "source": [
        "string = '''\n",
        "Sheldon Lee Cooper,[4][5] Ph.D., Sc.D.,[6] is a fictional character in the CBS television series The Big Bang Theory and its spinoff series Young Sheldon, portrayed by actors Jim Parsons and Iain Armitage respectively (with Parsons as the latter series' narrator).[7] For his portrayal, Parsons won four Primetime Emmy Awards, a Golden Globe Award, a TCA Award, and two Critics' Choice Television Awards. The character's childhood is the focus of Young Sheldon, in which he grows up in East Texas with his family Missy Cooper, George Cooper Sr., George Cooper Jr., Mary Cooper and his MeeMaw as a child prodigy.\n",
        "\n",
        "The adult Sheldon is a senior theoretical physicist at the California Institute of Technology (Caltech), and for the first ten seasons of The Big Bang Theory shares an apartment with his colleague and best friend, Leonard Hofstadter (Johnny Galecki); they are also friends and coworkers with Howard Wolowitz (Simon Helberg) and Rajesh Koothrappali (Kunal Nayyar). In season 10, Sheldon moves across the hall with his girlfriend Amy Farrah Fowler (Mayim Bialik), in the former apartment of Leonard's wife Penny (Kaley Cuoco).[8]\n",
        "\n",
        "He has a genius-level IQ of 187. In The Big Bang Theory, it is said that his and Leonard's IQs add up to 360, meaning Leonard has an IQ of 173. In Young Sheldon Season 7 Episode 7, when Sheldon was studying at home and was commanded to answer the phone, he became annoyed and stated that he is treated like a receptionist at home, despite having an IQ of 187, directly confirming the number. However, he displays a fundamental lack of social skills, a tenuous understanding of humor (always ending with \"bazinga\"), and difficulty recognizing irony and sarcasm in other people, although he himself often employs them. He exhibits highly idiosyncratic behavior and a general lack of humility, empathy, and toleration. These characteristics provide the majority of the humor involving him, which are credited with making him the show's breakout character.[9][10][11][12] Some viewers have asserted that Sheldon's personality is consistent with autism spectrum disorder (or what used to be classified as Asperger's Syndrome).[11][13] Co-creator Bill Prady has stated that Sheldon's character was neither conceived nor developed with regard to Asperger's,[13] although Parsons has said that in his opinion, Sheldon \"couldn't display more facets\" of Asperger's syndrome.[14]\n",
        "\n",
        "The character of Sheldon Cooper was inspired by a computer programmer personally known to series co-creator Bill Prady.[15] He and his friend Leonard Hofstadter are named in honor of actor/producer Sheldon Leonard[16] and Nobel Prize Laureate Leon Cooper.[17] Chuck Lorre originally intended Johnny Galecki to play the role, but Galecki thought he would be \"better suited\" for the character of Leonard.[18] Lorre said that when Jim Parsons auditioned for the role, he was \"so startlingly good\" that he was asked to reaudition \"to make sure he hadn't gotten lucky\".[19]\n",
        "\n",
        "Sheldon and his fraternal twin sister, Missy, were born on February 26, 1980, at Lawrence Memorial Hospital in Galveston, Texas,[20] and raised in Medford, a fictional small town in East Texas that is a three-hour drive from Dallas, along with their older brother, George Jr., by their mother, Mary Cooper, an overtly devout Baptist, and their father, George Cooper Sr., a football coach.[21] His first word was 'hypotenuse'; he said this at four months old.[22] Sheldon once got his father fired when he told Mr. Hinckley, a store owner, that George was stealing from the cash register.[23] In Young Sheldon, this is retconned: his father is a football coach who was fired from his coaching position in Galveston because he disclosed that other coaches were illegally recruiting players to their school, forcing the family to return to Medford.[24] He does drink, mostly beer, and is a loving father who is trying to understand his intellectually gifted son. The only member of his family to have actively encouraged his work in science was his maternal grandfather, whom he cherished and affectionately called \"Pop-Pop\", and who died when Sheldon was five years old. Pop-Pop's loss is what caused Sheldon to not like Christmas very much when his Christmas wish to bring Pop-Pop back did not come true. Sheldon's closest relative is his maternal grandmother whom he affectionately calls \"Meemaw\", and who in turn calls him \"Moon Pie\".[25] His aunt was also said to have encouraged his work in science by giving him medical equipment, \"in case his work in physics failed, he'd have a 'trade' to fall back on\". In Young Sheldon, it is shown that his childhood friend Tam was the one who introduced him to non-scientific interests such as comic books and Dungeons & Dragons.\n",
        "'''\n",
        "\n",
        "\n",
        "string = re.sub('\\[\\d*\\]','', string)\n",
        "string = re.sub('\\[\\w*\\]','', string)\n",
        "string = re.sub('\\[note \\d+\\]','', string)\n",
        "\n",
        "sents = sent_tokenize(string)\n",
        "C = CR_predictor.predict(string)\n",
        "clusters = C['clusters']\n",
        "doc = C['document']\n",
        "\n",
        "for cluster in clusters:\n",
        "  find_cluster(cluster, doc)\n",
        "  print('--------------------------------')\n",
        "\n",
        "sents_bounds = find_sents_bounds(string)\n",
        "\n",
        "sents_semantic_roles = []\n",
        "for sent in sents:\n",
        "  semantic_roles = SRL_predictor.predict(sent)\n",
        "  sents_semantic_roles.append(semantic_roles)\n",
        "\n",
        "\n",
        "sent_mappings = list()\n",
        "for sent_idx, sent in enumerate(sents):\n",
        "  sent_mapping = dict()\n",
        "  for word_idx, word in enumerate(sents_semantic_roles[sent_idx]['words']):\n",
        "    if word != '.':\n",
        "      word_start = find_in_sent(sents_semantic_roles[sent_idx]['words'], word_idx, sent, 'start')\n",
        "      sent_mapping[word_idx] = [word_start, word_start + len(word)]\n",
        "  sent_mappings.append(sent_mapping)\n",
        "\n",
        "chars_spans = []\n",
        "pronoun = None\n",
        "\n",
        "for spans in clusters:\n",
        "  chars_span = list()\n",
        "  for span in spans:\n",
        "    start_char = find(doc, span[0], mode='start')\n",
        "    end_char = find(doc, span[1], mode='end')\n",
        "    sent_idx = find_sent([start_char, end_char])\n",
        "    coref = string[start_char: end_char]\n",
        "    pronoun = find_pronoun(coref) if pronoun == None else pronoun\n",
        "    start_char_in_sent = start_char - sents_bounds[sent_idx][0]\n",
        "    end_char_in_sent = end_char - sents_bounds[sent_idx][0]\n",
        "\n",
        "    start_word_idx_in_sent = find_corresponding_word([start_char_in_sent, end_char_in_sent], sent_mappings[sent_idx], 'start')\n",
        "    end_word_idx_in_sent = find_corresponding_word([start_char_in_sent, end_char_in_sent], sent_mappings[sent_idx], 'end')\n",
        "\n",
        "    if not_possessive(coref):\n",
        "      chars_span.append([coref, sent_idx, start_word_idx_in_sent, end_word_idx_in_sent])\n",
        "  chars_spans.append(chars_span)\n",
        "\n",
        "cluster_names = []\n",
        "for cluster in chars_spans:\n",
        "  cluster_names.append([coref[0] for coref in cluster])\n",
        "\n",
        "cluster_2_sent = dict()\n",
        "for cluster_idx, cluster in enumerate(chars_spans):\n",
        "  cluster_2_sent[cluster_idx] = set()\n",
        "  for coref in cluster:\n",
        "    cluster_2_sent[cluster_idx].add(coref[1])\n",
        "  cluster_2_sent[cluster_idx] = list(cluster_2_sent[cluster_idx])\n",
        "\n",
        "# constructing graph\n",
        "nodes = list()\n",
        "for sent_idx in range(len(sents)):\n",
        "  clusters_2_put = list()\n",
        "  for cluster_idx, cluster in enumerate(chars_spans):\n",
        "\n",
        "    if sent_idx in cluster_2_sent[cluster_idx]:\n",
        "      clusters_2_put.append(cluster_idx)\n",
        "  nodes.append(Node(sent_idx, clusters_2_put))\n",
        "\n",
        "for node_1_idx, node_1 in enumerate(nodes):\n",
        "  for node_2_idx, node_2 in enumerate(nodes):\n",
        "    if node_1_idx != node_2_idx:\n",
        "      intersects = list(set(node_1.coref_clusters) & set(node_2.coref_clusters))\n",
        "\n",
        "for node_1_idx, node_1 in enumerate(nodes):\n",
        "  for node_2_idx, node_2 in enumerate(nodes):\n",
        "    if node_1_idx != node_2_idx:\n",
        "      intersects = list(set(node_1.coref_clusters) & set(node_2.coref_clusters))\n",
        "      if intersects != []:\n",
        "        for intersect in intersects:\n",
        "          node_1.set_edge(node_2_idx, intersect)\n",
        "  node_1.make_cluster_2_sent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRjHhFPMuUCk"
      },
      "outputs": [],
      "source": [
        "G = nx.Graph()\n",
        "\n",
        "for node_idx, node in enumerate(nodes):\n",
        "  G.add_node(node_idx)\n",
        "\n",
        "for node_idx, node in enumerate(nodes):\n",
        "  for edge in node.edges:\n",
        "    G.add_edge(node_idx, edge[0])\n",
        "\n",
        "pos = nx.spring_layout(G)\n",
        "nx.draw(G, pos=pos, with_labels = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More Functions"
      ],
      "metadata": {
        "id": "MG9B9RDSmQvh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXpvpGMLI44k"
      },
      "outputs": [],
      "source": [
        "def find_verbs(pos_tags):\n",
        "  verbs = list()\n",
        "  for pos_tag in pos_tags:\n",
        "    if pos_tag[1].startswith('V'):\n",
        "      verbs.append(pos_tag)\n",
        "  return verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0Cb0eV_J59L"
      },
      "outputs": [],
      "source": [
        "def del_relatives(sent):\n",
        "  words = sent.split()\n",
        "  pos_tags = nltk.pos_tag(words)\n",
        "  for tag_idx, pos_tag in enumerate(pos_tags):\n",
        "    if pos_tag[1] == 'WRB' or pos_tag[1] == 'WDT' or pos_tag[1] == 'WP':\n",
        "      return ' '.join(words[:tag_idx])\n",
        "  return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wEi4RFhT3aO"
      },
      "outputs": [],
      "source": [
        "def clean_question(sent):\n",
        "  words = sent.split()\n",
        "  pos_tags = nltk.pos_tag(words)\n",
        "  sent = re.sub('\\(\\s*\\w*\\s*\\)', '', sent)\n",
        "  return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZk6UdnTCapq"
      },
      "outputs": [],
      "source": [
        "def filter_answer(answer):\n",
        "  try:\n",
        "    pos_tags = nltk.pos_tag(answer.split())\n",
        "    if pos_tags[0][1] == 'IN':\n",
        "      new_answer = ' '.join(answer.split()[1: ])\n",
        "    else:\n",
        "      new_answer = answer\n",
        "  except:\n",
        "    return answer\n",
        "\n",
        "  return new_answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVr4W-i3EMyr"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "  text = text.strip()\n",
        "  while(True):\n",
        "    if text.startswith(',') or text.startswith(';') or text.startswith(':'):\n",
        "      text = text[1: ]\n",
        "    else:\n",
        "      break\n",
        "\n",
        "  while(True):\n",
        "    if text.endswith(',') or text.endswith(';') or text.endswith(':'):\n",
        "      text = text[ :-1]\n",
        "    else:\n",
        "      break\n",
        "  return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DSDiid9mL1k"
      },
      "outputs": [],
      "source": [
        "from pattern.en import conjugate, lemma, lexeme, PRESENT, SG, PAST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDMl5OW3bHFR"
      },
      "outputs": [],
      "source": [
        "def extra_check_person(x, q_wh):\n",
        "  if x.lower().strip() in ['i', 'she', 'he', 'you', 'we', 'they', 'me', 'him', 'her', 'us']:\n",
        "    return 'who'\n",
        "  else:\n",
        "    return q_wh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nUeJK8ZOb8kz"
      },
      "outputs": [],
      "source": [
        "def convert_to_object(pronoun):\n",
        "  pronouns_mapping = {\n",
        "      'i': 'me',\n",
        "      'you': 'you',\n",
        "      'he': 'him',\n",
        "      'she': 'her',\n",
        "      'we': 'us',\n",
        "      'it': 'it',\n",
        "      'they': 'them'\n",
        "  }\n",
        "\n",
        "  if pronoun.lower() in pronouns_mapping.keys():\n",
        "    return pronouns_mapping[pronoun.lower()]\n",
        "  else:\n",
        "    return pronoun"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPitb9uYvBlO"
      },
      "outputs": [],
      "source": [
        "def find_main_verb_pos(verb_idx):\n",
        "\n",
        "  def find_in_doc(doc, word_text):\n",
        "    for word_idx, word in enumerate(doc):\n",
        "      if word_text == word[0]:\n",
        "        return word[1]\n",
        "\n",
        "  if doc[verb_idx].text == doc[verb_idx].head.text:\n",
        "    return pos_tags[verb_idx]\n",
        "  else:\n",
        "    return find_in_doc(pos_tags, doc[verb_idx].head.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2K8Vpdj5xSi"
      },
      "outputs": [],
      "source": [
        "def has_aux(sent, verb_idx):\n",
        "  pos_tags = nltk.pos_tag(word_tokenize(sent))\n",
        "  doc_ = nlp(sent)\n",
        "\n",
        "  for child in doc_[verb_idx].children:\n",
        "    if 'aux' in child.dep_:\n",
        "      return child.i\n",
        "    elif 'advcl' in child.dep_:\n",
        "      for child_ in child.children:\n",
        "        if 'aux' in child_.dep_:\n",
        "          return child_.i\n",
        "\n",
        "  head = doc_[verb_idx].head\n",
        "  if 'advcl' == head.dep_:\n",
        "    for child in head.children:\n",
        "      if 'aux' in child.dep_:\n",
        "        return child.i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_u4DUVlvVJT"
      },
      "outputs": [],
      "source": [
        "def get_main_verb_idx(verb_idx, sent):\n",
        "\n",
        "  def find_in_doc(doc, word_text):\n",
        "    for word_idx, word in enumerate(doc):\n",
        "      if word_text == word[0] and (doc_[word_idx].pos_ == 'VERB' or doc_[word_idx].pos_ == 'AUX'):\n",
        "        return word_idx\n",
        "\n",
        "\n",
        "  doc_ = nlp(sent)\n",
        "  aux_idx = has_aux(sent, verb_idx)\n",
        "  if aux_idx:\n",
        "    return aux_idx\n",
        "\n",
        "\n",
        "  if doc_[verb_idx].text == doc_[verb_idx].head.text:\n",
        "    return verb_idx\n",
        "  else:\n",
        "    A = find_in_doc(pos_tags, doc_[verb_idx].head.text)\n",
        "    if A:\n",
        "      return A\n",
        "    else:\n",
        "      return verb_idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wy1q7ej57jc"
      },
      "outputs": [],
      "source": [
        "def get_non_trivial_coref_(cluster_idx):\n",
        "  cluster_ = chars_spans[cluster_idx]\n",
        "  corefs = [coref[0] for coref in cluster_]\n",
        "  non_trivial_corefs = []\n",
        "  main_pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'its', 'itself', 'himself', 'herself', 'myself', 'yourself', 'themselves']\n",
        "  for coref in corefs:\n",
        "    if coref.lower() not in main_pronouns:\n",
        "      print(coref)\n",
        "      non_trivial_corefs.append(coref)\n",
        "  if len(non_trivial_corefs) > 0:\n",
        "    return sample(non_trivial_corefs)\n",
        "  else:\n",
        "    return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbVJrbuM3tWx"
      },
      "outputs": [],
      "source": [
        "def get_non_trivial_coref(corefs):\n",
        "  non_trivial_corefs = []\n",
        "  main_pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'its', 'itself', 'himself', 'herself', 'myself', 'yourself', 'themselves']\n",
        "  for coref in corefs:\n",
        "    if coref.lower() not in main_pronouns:\n",
        "      non_trivial_corefs.append(coref)\n",
        "  if len(non_trivial_corefs) > 0:\n",
        "    return sample(non_trivial_corefs)\n",
        "  else:\n",
        "    return ''\n",
        "\n",
        "\n",
        "def get_trivial_coref(corefs):\n",
        "  main_pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'its']\n",
        "  for coref in corefs:\n",
        "    if coref.lower() in main_pronouns:\n",
        "      return coref\n",
        "  return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQo_KAnV5rbG"
      },
      "outputs": [],
      "source": [
        "def simple_pronoun(ent):\n",
        "  pronouns = ['i', 'you', 'it', 'he', 'she', 'we', 'they', 'me', 'him', 'her', 'our', 'them']\n",
        "  if ent.lower().strip() in pronouns:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdzfeCTDwyb5"
      },
      "outputs": [],
      "source": [
        "def make_agent_in_the_right_form(agent):\n",
        "  transforms = {\n",
        "      'him': 'he',\n",
        "      'her': 'she',\n",
        "      'them': 'they',\n",
        "      'me': 'i',\n",
        "      'us': 'we'\n",
        "  }\n",
        "  agent = agent.strip()\n",
        "  agent_tokens = agent.split(' ')\n",
        "  try:\n",
        "    if agent_tokens[0] in ['by', 'with']:\n",
        "      agent = ' '.join(agent_tokens[1: ])\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  if agent.lower() in transforms.keys():\n",
        "    return transforms[agent.lower()]\n",
        "  else:\n",
        "    return agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hSnNaG1vY2h"
      },
      "outputs": [],
      "source": [
        "def filter_aps(word):\n",
        "  banned_words = ['when', 'where', 'what', 'who', 'from', 'to']\n",
        "  if word.lower() in banned_words:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn65bpnyGydi"
      },
      "outputs": [],
      "source": [
        "def has_aux(sent, verb_idx):\n",
        "  pos_tags = nltk.pos_tag(word_tokenize(sent))\n",
        "  doc_ = nlp(sent)\n",
        "  for child in doc_[verb_idx].children:\n",
        "    if 'aux' in child.dep_:\n",
        "      return child.i\n",
        "    elif 'advcl' in child.dep_:\n",
        "      for child_ in child.children:\n",
        "        if 'aux' in child_.dep_:\n",
        "          return child_.i\n",
        "  head = doc_[verb_idx].head\n",
        "  if 'advcl' == doc_[verb_idx].dep_:\n",
        "    for child in head.children:\n",
        "      if 'aux' in child.dep_:\n",
        "        return child.i\n",
        "\n",
        "  if head.pos_ == 'AUX':\n",
        "    return head.text\n",
        "\n",
        "  last_try = find_remained(sent)\n",
        "  if last_try is not None:\n",
        "    return last_try[0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.chunk import conlltags2tree, tree2conlltags\n",
        "from nltk.corpus import conll2000\n",
        "\n",
        "# Ensure necessary NLTK data files are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('conll2000')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "uF1CjOljvBq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FkB7GZtku82n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dwUxC3A4u85o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2T9VfQFMu88Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qjW-b_Nmu8_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qkgv5c20u9C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hb4IphxtKDvv"
      },
      "outputs": [],
      "source": [
        "def get_aux(sent, verb_idx):\n",
        "  pos_tags = nltk.pos_tag(word_tokenize(sent))\n",
        "  doc_ = nlp(sent)\n",
        "  for child in doc_[verb_idx].children:\n",
        "    if 'aux' in child.dep_:\n",
        "      return child.text\n",
        "    elif 'advcl' in child.dep_:\n",
        "      for child_ in child.children:\n",
        "        if 'aux' in child_.dep_:\n",
        "          return child_.text\n",
        "\n",
        "  head = doc_[verb_idx].head\n",
        "  if 'advcl' == doc_[verb_idx].dep_:\n",
        "    for child in head.children:\n",
        "      if 'aux' in child.dep_:\n",
        "        return child.text\n",
        "\n",
        "\n",
        "  if head.pos_ == 'AUX':\n",
        "    return head.text\n",
        "\n",
        "\n",
        "  last_try = find_remained(sent)\n",
        "  if last_try is not None:\n",
        "    return last_try[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "GziVbPa8KQ7r"
      },
      "outputs": [],
      "source": [
        "def find_remained(sent):\n",
        "  doc = nlp(sent)\n",
        "  pos_tags = nltk.pos_tag(word_tokenize(sent))\n",
        "  for token_idx, token in enumerate(doc):\n",
        "    if token.pos_ == 'VERB' and token.tag_ == 'VBN':\n",
        "      for child in token.children:\n",
        "        if 'AUX' in child.dep_.upper():\n",
        "          return child.i, child.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "UmOD2-nHC2Op"
      },
      "outputs": [],
      "source": [
        "def print_pos(text):\n",
        "  print(nltk.pos_tag(word_tokenize(text)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "apuIk2t6PS9f"
      },
      "outputs": [],
      "source": [
        "def find_IN(text):\n",
        "  tokens = nltk.pos_tag(word_tokenize(text))\n",
        "  for token in tokens:\n",
        "    if token[1] == 'IN':\n",
        "      return token[0]\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "lQwIQTz4C2RI"
      },
      "outputs": [],
      "source": [
        "def print_srl(text, idx):\n",
        "  print(SRL_predictor.predict(text)['verbs'][idx]['description'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Zqju5dOoQjqC"
      },
      "outputs": [],
      "source": [
        "A = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "CAyM2f7z8puX"
      },
      "outputs": [],
      "source": [
        "def get_non_trivial_coref(corefs):\n",
        "  main_pronouns = ['i', 'you', 'he', 'she', 'it', 'we', 'they', 'its', 'itself', 'himself', 'herself', 'myself', 'yourself', 'themselves']\n",
        "  for coref in corefs:\n",
        "    if coref[0].lower() not in main_pronouns:\n",
        "      return coref[0]\n",
        "  return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "Hp4vsj5VQsGS"
      },
      "outputs": [],
      "source": [
        "A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "_bLxXtlVtr3x"
      },
      "outputs": [],
      "source": [
        "def filter_specific(srs):\n",
        "  for sr in srs:\n",
        "    if sr in ['who', 'self', 'which', 'that']:\n",
        "      return False\n",
        "  return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "W2mRHK063sE8"
      },
      "outputs": [],
      "source": [
        "def past_sr(sent):\n",
        "  for token in nltk.pos_tag(word_tokenize(sent)):\n",
        "    if token[1] == 'VBD':\n",
        "      return True\n",
        "  return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "P5kJ0HUGCpgz"
      },
      "outputs": [],
      "source": [
        "def find_extra_verb(sent, verb_idx):\n",
        "  doc = nlp(sent)\n",
        "  verb = doc[verb_idx]\n",
        "  X = False\n",
        "\n",
        "  for child in verb.children:\n",
        "    if 'aux' in child.dep_.lower():\n",
        "      X = True\n",
        "\n",
        "  if (doc[verb_idx].dep_ == 'xcomp' or doc[verb_idx].dep_ == 'ccomp') and doc[verb_idx].head.pos_ == 'VERB' and (verb.tag_ == 'VBG' or X):\n",
        "    return doc[verb_idx].head.text\n",
        "  else:\n",
        "    return ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "LUY-jpfsRwer"
      },
      "outputs": [],
      "source": [
        "def has_past(sent):\n",
        "  pos_tags = nltk.pos_tag(word_tokenize(sent))\n",
        "  for pos_tag in pos_tags:\n",
        "    if pos_tag[1] == 'VBD' or (pos_tag[1] == 'VBZ' and pos_tag[0] not in ['is', 'are', 'am']):\n",
        "      return True\n",
        "  return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "du8v-rAP-nEx"
      },
      "outputs": [],
      "source": [
        "# determines the singularity or plurality of a noun\n",
        "def is_single(noun):\n",
        "  if 'and' in noun:\n",
        "    return False\n",
        "\n",
        "  if inflect.singular_noun(noun) == False:\n",
        "    return True\n",
        "  else:\n",
        "    return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "quLIcP4omyJW"
      },
      "outputs": [],
      "source": [
        "def det_wh_based_on_ent(ent):\n",
        "  if 'PER' in ent:\n",
        "    return 'who'\n",
        "  elif 'LOC' in ent:\n",
        "    return 'where'\n",
        "  elif 'ORG' in ent:\n",
        "    return 'what'\n",
        "  elif 'O' in ent:\n",
        "    return 'what'\n",
        "  else:\n",
        "    return 'what'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "W1u5pTvJlxvM"
      },
      "outputs": [],
      "source": [
        "def has_noun_ancestor(aswer_ents, doc, idx):\n",
        "  x = doc[idx]\n",
        "  last_N_idx = idx\n",
        "  while(True):\n",
        "    if x.dep_ == 'ROOT':\n",
        "      if 'NN' in x.tag_:\n",
        "        return det_wh_based_on_ent(aswer_ents[x.i])\n",
        "      else:\n",
        "        return det_wh_based_on_ent(aswer_ents[last_N_idx])\n",
        "    else:\n",
        "      if 'NN' in x.tag_:\n",
        "        last_N_idx = x.i\n",
        "    x = x.head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "J4PaI_S-lJPG"
      },
      "outputs": [],
      "source": [
        "def sample(x):\n",
        "  return random.sample(x, k=1)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "9WKsaTcq9i6r"
      },
      "outputs": [],
      "source": [
        "def find_wh_question(ent_types, srl_type, aswer_ents, answer):\n",
        "\n",
        "  doc = nlp(answer)\n",
        "\n",
        "  for ent, word in zip(aswer_ents, doc):\n",
        "    if word.dep_ == 'ROOT':\n",
        "      if 'PER' in ent:\n",
        "        return 'who'\n",
        "      elif 'LOC' in ent:\n",
        "        return 'where'\n",
        "      elif 'ORG' in ent:\n",
        "        return 'what'\n",
        "\n",
        "  for word_idx, (ent, word) in enumerate(zip(aswer_ents, doc)):\n",
        "    if 'LOC' in ent or 'PER' in ent or 'ORG' in ent:\n",
        "      return has_noun_ancestor(aswer_ents, doc, word_idx)\n",
        "\n",
        "  return 'what'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "o9aPwR1V1qoA"
      },
      "outputs": [],
      "source": [
        "clusters_person = []\n",
        "cluster_trivial_coref = []\n",
        "\n",
        "for cluster_idx, cluster in enumerate(chars_spans):\n",
        "  is_person = False\n",
        "  trivial_coref = ''\n",
        "  for coref in cluster:\n",
        "    if coref[0].lower() in ['he', 'she', 'her', 'him', 'i', 'my', 'his', 'hers']:\n",
        "      is_person = True\n",
        "      if coref[0].lower() in ['he', 'him', 'his']:\n",
        "        trivial_coref = 'he'\n",
        "      if coref[0].lower() in ['she', 'her', 'hers']:\n",
        "        trivial_coref = 'she'\n",
        "      if coref[0].lower() in ['we', 'us', 'our', 'ours']:\n",
        "        trivial_coref = 'we'\n",
        "      if coref[0].lower() in ['it', 'its']:\n",
        "        trivial_coref = 'it'\n",
        "      if coref[0].lower() in ['they', 'them', 'their', 'theirs']:\n",
        "        trivial_coref = 'they'\n",
        "\n",
        "  if is_person:\n",
        "    clusters_person.append(True)\n",
        "  else:\n",
        "    clusters_person.append(False)\n",
        "\n",
        "  cluster_trivial_coref.append(trivial_coref)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "UkJPWLVLLVS2"
      },
      "outputs": [],
      "source": [
        "def give_new_ents(arg, arg_indexes, arg_cluster_idx, ents):\n",
        "  all_ents = deepcopy(ents)\n",
        "  if arg_cluster_idx is not None:\n",
        "    for coref in chars_spans[arg_cluster_idx]:\n",
        "      if coref[1] == sent_idx and clusters_person[arg_cluster_idx]:\n",
        "        if coref[2] == coref[3]:\n",
        "          all_ents[coref[2]] = 'U-PER'\n",
        "        else:\n",
        "          all_ents[coref[2]] = 'B-PER'\n",
        "          for jdx in range(coref[2] + 1, coref[3] + 1):\n",
        "            all_ents[coref[2]] = 'L-PER'\n",
        "\n",
        "  if len(all_ents[arg_indexes[0]: arg_indexes[1] + 1]) == 1 and len(arg.split(' ')) > 1:\n",
        "    Y = all_ents[arg_indexes[0]: arg_indexes[1] + 1] * len(arg.split(' '))\n",
        "  else:\n",
        "    Y = all_ents[arg_indexes[0]: arg_indexes[1] + 1]\n",
        "\n",
        "  return Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "UHp_jutValGQ"
      },
      "outputs": [],
      "source": [
        "def get_corefs(arg, arg_indexes, coref_indexes, cluster_idx, mode):\n",
        "  words = word_tokenize(arg)\n",
        "\n",
        "  main_indexes = [coref_indexes[0] - arg_indexes[0], coref_indexes[1] - arg_indexes[0]]\n",
        "\n",
        "  if mode == 'non_trivial':\n",
        "    new_coref = get_non_trivial_coref([coref[0] for coref in chars_spans[cluster_idx]])\n",
        "  if mode == 'trivial':\n",
        "    new_coref = get_trivial_coref([coref[0] for coref in chars_spans[cluster_idx]])\n",
        "\n",
        "  coref_words = word_tokenize(new_coref)\n",
        "\n",
        "  new_words = words[0: main_indexes[0] + 1] + coref_words + words[main_indexes[1]: ]\n",
        "\n",
        "  new_words = ' '.join(new_words)\n",
        "\n",
        "\n",
        "  return new_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "cm3aNL64yQGC"
      },
      "outputs": [],
      "source": [
        "def not_suitable(text):\n",
        "  list_ = ['to', 'when', 'where', 'who', 'the', 'what', 'from', 'in', 'at']\n",
        "  return True if text.lower() in list_ else False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a chunk grammar to identify verb phrases (VP)\n",
        "grammar = r\"\"\"\n",
        "    VP: {<MD>?<VB.*><RB|RP|DT|IN|PRP|JJ>*<VB.*|VBN|VBG>*}\n",
        "\"\"\"\n",
        "\n",
        "def extract_verb_auxiliaries(sentence, target_verb):\n",
        "    # Tokenize the sentence\n",
        "    tokens = word_tokenize(sentence)\n",
        "    # Get the part-of-speech tags for the tokens\n",
        "    pos_tags = pos_tag(tokens)\n",
        "\n",
        "    # Parse the sentence to extract verb phrases (VP)\n",
        "    cp = nltk.RegexpParser(grammar)\n",
        "    tree = cp.parse(pos_tags)\n",
        "\n",
        "    # Extract auxiliary verbs associated with the target verb\n",
        "    aux_verbs = []\n",
        "    for subtree in tree:\n",
        "        if isinstance(subtree, Tree) and subtree.label() == 'VP':\n",
        "            words = [word for word, tag in subtree.leaves()]\n",
        "            if target_verb in words:\n",
        "                aux_verbs = [word for word, tag in subtree.leaves() if tag in ('MD', 'VBZ', 'VBD', 'VBG', 'VBN', 'VBP') and word != target_verb]\n",
        "                break\n",
        "\n",
        "    return ' '.join(aux_verbs)\n",
        "\n",
        "# Example usage\n",
        "sentence = \"He would have been performing it\"\n",
        "target_verb = \"performing\"\n",
        "auxiliary_verbs = extract_verb_auxiliaries(sentence, target_verb)\n",
        "print(\"Auxiliary Verbs for '{}':\".format(target_verb), auxiliary_verbs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP8eWIDrs1oq",
        "outputId": "b14d296e-e6e4-452b-b351-7d67702febba"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auxiliary Verbs for 'performing': would been\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sentence_root(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    root = None\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ == 'ROOT':\n",
        "            root = token\n",
        "            break\n",
        "    return root"
      ],
      "metadata": {
        "id": "BaiIOWszjovp"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb-j3QLP5eQA"
      },
      "source": [
        "# Unit Test Agent & Patient"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "2IHrq2ZHmp-W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1b306a-8e94-4676-90f6-2efc3bcff0c6"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_verb_lemma(verb):\n",
        "  try:\n",
        "    verb = lemma(verb)\n",
        "  except:\n",
        "    import_error()\n",
        "    verb = lemma(verb)\n",
        "  return verb\n",
        "\n",
        "def plural_pronoun(pronoun):\n",
        "    if pronoun.lower() in ['i', 'me', 'she', 'her', 'he', 'him', 'it']:\n",
        "      return False\n",
        "    elif pronoun.lower() in ['we', 'us', 'they', 'them']:\n",
        "      return True\n",
        "\n",
        "\n",
        "def is_plural(sent, word_idx):\n",
        "\n",
        "    tokenized_word = nltk.word_tokenize(sent)\n",
        "    pos_tags = nltk.pos_tag(tokenized_word)\n",
        "    word = pos_tags[word_idx][0]\n",
        "    word_pos_tag = pos_tags[word_idx][1]\n",
        "\n",
        "    if word_pos_tag in ['NNS', 'NNPS']:\n",
        "        return True\n",
        "    elif word_pos_tag in ['NN', 'NNP']:\n",
        "        return False\n",
        "    elif word_pos_tag in ['PRP']:\n",
        "        return plural_pronoun(word)"
      ],
      "metadata": {
        "id": "14rhwFlfCWbY"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "is_plural('troubles want us', 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNxD_RPAELeD",
        "outputId": "228d2db0-7381-4415-c038-6df2f68a9413"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# need to be working further, discriminate does and do\n",
        "def get_q_ext(words, verb_indexes):\n",
        "  pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "  try:\n",
        "    verb_pos = pos_tags[verb_indexes[0]][1]\n",
        "  except:\n",
        "    verb_pos = 'VB'\n",
        "\n",
        "  if verb_pos == 'VBD' or verb_pos == 'VBN':\n",
        "    q_ext = 'did'\n",
        "  elif verb_pos == 'VB' or verb_pos == 'VBZ' or verb_pos == 'VBG' or verb_pos == 'VBP' or verb_pos == 'MD':\n",
        "    if inflect.singular_noun(verb) == False:\n",
        "        q_ext = 'do'\n",
        "    else:\n",
        "        q_ext = 'does'\n",
        "  else:\n",
        "    q_ext = 'did'\n",
        "\n",
        "\n",
        "  return q_ext"
      ],
      "metadata": {
        "id": "IWZgqGdMD5kn"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_final_text(string):\n",
        "  final_text = ''\n",
        "  prev_word = ''\n",
        "  apos_count = 0\n",
        "  words = string.split()\n",
        "  for word in words:\n",
        "    if word == '\\\"':\n",
        "      apos_count += 1\n",
        "    if word.startswith('\\'') or word.startswith('n\\'t') or word.startswith(',') or word.startswith('.') or \\\n",
        "      word.startswith(':') or word.startswith('?') or word.startswith('!') or word.startswith(';'):\n",
        "      final_text += word\n",
        "    elif word.startswith('-') or prev_word.startswith('-'):\n",
        "      final_text += word\n",
        "    elif prev_word.startswith('(') or word.startswith(')'):\n",
        "      final_text += word\n",
        "    elif prev_word == '\\\"' and (apos_count + 1) % 2 == 0:\n",
        "      final_text += word\n",
        "    elif word == '\\\"' and apos_count % 2 == 0:\n",
        "      final_text += word\n",
        "    else:\n",
        "      final_text += ' ' + word\n",
        "    prev_word = word\n",
        "  return final_text.strip()"
      ],
      "metadata": {
        "id": "OrZQbaHZsDIC"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_2_cluster = dict()\n",
        "for idx, cluster_name in enumerate(cluster_names):\n",
        "  idx_2_cluster[idx] = list(set(cluster_name))"
      ],
      "metadata": {
        "id": "-_8HD5nZYjKu"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pronoun(pns, simple=True):\n",
        "  found_simple = False\n",
        "  non_simple = []\n",
        "  if simple:\n",
        "    for pn in pns:\n",
        "      if pn in pronouns:\n",
        "        found_simple = True\n",
        "        return pn.lower()\n",
        "\n",
        "  elif not simple:\n",
        "    for pn in pns:\n",
        "      if not pn in pronouns:\n",
        "        non_simple.append(pn)\n",
        "    return random.sample(non_simple, k=1)[0].lower()\n",
        "\n",
        "  if simple and not found_simple:\n",
        "    return random.sample(pns, k=1)[0].lower()"
      ],
      "metadata": {
        "id": "aZWhuackZJCe"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_aux(doc, verb_idx):\n",
        "\n",
        "    token = doc[verb_idx]\n",
        "    aux_verbs = []\n",
        "    # Collect auxiliary verbs\n",
        "    for child in token.children:\n",
        "        if child.dep_ in ['aux', 'auxpass']:\n",
        "            aux_verbs.append(child.text)\n",
        "        elif child.dep_ == 'neg' and 'n\\'t' in child.text:\n",
        "            aux_verbs[-1] += child.text\n",
        "        elif child.dep_ == 'neg' and child.text == 'not':\n",
        "            aux_verbs.append(child.text)\n",
        "\n",
        "    aux_verbs.append(token.text)\n",
        "\n",
        "    # for child in token.children:\n",
        "    #     if child.dep_ == 'xcomp' and child.pos_ == 'VERB':\n",
        "    #         print('ttt', child.text)\n",
        "    #         aux_verbs.append('to ' + child.text)\n",
        "    # print(aux_verbs)\n",
        "    return ' '.join(aux_verbs[ :-1])"
      ],
      "metadata": {
        "id": "qq1v5eSGSIAn"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = '''Chuck Lorre originally intended Johnny Galecki to play the role, but Galecki thought he would be \"better suited\" for the character of Leonard.'''\n",
        "sent_doc = nlp(sent)"
      ],
      "metadata": {
        "id": "u-owkv1VFaCj"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_aux(sent_doc, 3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MRGHLbHKFsfW",
        "outputId": "f3448f08-7a02-4a42-cbe2-99a96121e4a5"
      },
      "execution_count": 79,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjVqFBhRziAy",
        "outputId": "e8ffd0aa-4bee-41c1-b92f-0d907bc062e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 - what did Parsons win For his portrayal? four Primetime Emmy Awards, a Golden Globe Award, a TCA Award, and two Critics' Choice Television Awards\n",
            "1 - what does He have? a genius-level IQ of 187\n",
            "2 - what does Leonard have? an IQ of 173\n",
            "3 - what did he state when Sheldon was studying at home and was commanded to answer the phone In Young Sheldon Season 7 Episode 7? that he is treated like a receptionist at home, despite having an IQ of 187, directly confirming the number\n",
            "4 - what does he display? a fundamental lack of social skills, a tenuous understanding of humor (always ending with \"bazinga\"), and difficulty recognizing irony and sarcasm in other people\n",
            "5 - what does he himself employ often? irony and sarcasm\n",
            "6 - what does He exhibit? highly idiosyncratic behavior and a general lack of humility, empathy, and toleration\n",
            "7 - what does These characteristics provide? the majority of the humor involving him, which are credited with making him the show's breakout character\n",
            "8 - what have Some viewers asserted? that Sheldon's personality is consistent with autism spectrum disorder (or what used to be classified as Asperger's Syndrome)\n",
            "9 - what has Co-creator Bill Prady stated? that Sheldon's character was neither conceived nor developed with regard to Asperger's\n",
            "10 - what has Parsons said? that in his opinion, Sheldon \"couldn't display more facets\" of Asperger's syndrome\n",
            "11 - what couldn't Sheldon display? more facets \"of Asperger's syndrome\n",
            "12 - what was by a computer programmer personally known to series co-creator Bill Prady inspired? The character of Sheldon Cooper\n",
            "13 - what did Chuck Lorre intend originally? Johnny Galecki to play the role\n",
            "14 - what did Galecki think? he would be \"better suited\" for the character of Leonard\n",
            "15 - who did Lorre say? that when Jim Parsons auditioned for the role, he was \"so startlingly good\" that he was asked to reaudition \"to make sure he hadn't gotten lucky\"\n",
            "16 - what did Sheldon get once when he told Mr. Hinckley, a store owner, that George was stealing from the cash register? his father fired\n",
            "17 - what did he tell Mr. Hinckley, a store owner, when? that George was stealing from the cash register\n",
            "18 - what did he disclose? that other coaches were illegally recruiting players to their school, forcing the family to return to Medford\n",
            "19 - what to have The only member of his family encouraged? his work in science\n",
            "20 - what did he cherish? his maternal grandfather\n",
            "21 - what did he call \"Pop-Pop\"? his maternal grandfather\n",
            "22 - what did Pop-Pop's loss cause? Sheldon to not like Christmas very much when his Christmas wish to bring Pop-Pop back did not come true\n",
            "23 - what does his wish Christmas? to bring Pop-Pop back\n",
            "24 - what does he call \"Meemaw\"? his maternal grandmother\n",
            "25 - who does his maternal grandmother call \"Moon Pie\"? Sheldon Lee Cooper, Ph.D., Sc.D.\n",
            "26 - what to have His aunt encouraged? his work in science\n",
            "27 - what'd he have? a' trade' to fall back on\n",
            "28 - who did the one introduce to non-scientific interests such as comic books and Dungeons & Dragons? Sheldon Lee Cooper, Ph.D., Sc.D.\n"
          ]
        }
      ],
      "source": [
        "agent_list = []\n",
        "Qs = []\n",
        "\n",
        "Agent_Patient_Qs = []\n",
        "nidx = 0\n",
        "\n",
        "for sent_idx, sent in enumerate(sents):\n",
        "  sent = sents[sent_idx]\n",
        "  sent_semantic = sents_semantic_roles[sent_idx]\n",
        "  words = sent_semantic['words']\n",
        "  entities = extract_entities(sent)\n",
        "  all_ents = NER_predictor.predict(sent)['tags']\n",
        "  sent_doc = nlp(sent)\n",
        "\n",
        "  for idx in range(len(sent_semantic['verbs'])):\n",
        "    semantic_tags = sent_semantic['verbs'][idx]['tags']\n",
        "    pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "    # Find all semantics\n",
        "    ext, ext_exists, ext_indexes = find_arg(words, semantic_tags, 'EXT')\n",
        "    time, time_exists, time_indexes = find_arg(words, semantic_tags, 'TMP')\n",
        "    manner, manner_exists, manner_indexes  = find_arg(words, semantic_tags, 'MNR')\n",
        "    agent, agent_exists, agent_indexes = find_arg(words, semantic_tags, 'ARG0')\n",
        "    patient, patient_exists, patient_indexes = find_arg(words, semantic_tags, 'ARG1')\n",
        "    state, state_exists, state_indexes = find_arg(words, semantic_tags, 'ARG2')\n",
        "    arg3, arg3_exists, arg3_indexes = find_arg(words, semantic_tags, 'ARG3')\n",
        "    arg4, arg4_exists, arg4_indexes = find_arg(words, semantic_tags, 'ARG4')\n",
        "    cause, cause_exists, cause_indexes = find_arg(words, semantic_tags, 'CAU')\n",
        "    loc, loc_exists, loc_indexes = find_arg(words, semantic_tags, 'LOC')\n",
        "    adv, adv_exists, adv_indexes = find_arg(words, semantic_tags, 'ADV')\n",
        "    verb, verb_exists, verb_indexes = find_arg(words, semantic_tags, '-V')\n",
        "\n",
        "    # find clusters\n",
        "    ext_corefs, ext_cluster_idx = find_corresponding_coref(ext_indexes, sent_idx)\n",
        "    time_corefs, time_cluster_idx = find_corresponding_coref(time_indexes, sent_idx)\n",
        "    manner_corefs, manner_cluster_idx = find_corresponding_coref(manner_indexes, sent_idx)\n",
        "    agent_corefs, agent_cluster_idx = find_corresponding_coref(agent_indexes, sent_idx)\n",
        "    patient_corefs, patient_cluster_idx = find_corresponding_coref(patient_indexes, sent_idx)\n",
        "    state_corefs, state_cluster_idx = find_corresponding_coref(state_indexes, sent_idx)\n",
        "    cause_corefs, cause_cluster_idx = find_corresponding_coref(cause_indexes, sent_idx)\n",
        "    loc_corefs, loc_cluster_idx = find_corresponding_coref(loc_indexes, sent_idx)\n",
        "\n",
        "    # if there is no verb continue\n",
        "    if len(verb_indexes) == 0:\n",
        "      continue\n",
        "\n",
        "    verb_index = verb_indexes[0]\n",
        "\n",
        "\n",
        "    semantic_role_ent_type = find_semantic_ent_type(entities,\n",
        "                             [ext_indexes,\n",
        "                              time_indexes,\n",
        "                              manner_indexes,\n",
        "                              agent_indexes,\n",
        "                              patient_indexes,\n",
        "                              state_indexes,\n",
        "                              cause_indexes,\n",
        "                              loc_indexes])\n",
        "\n",
        "\n",
        "    ext_ent_types, time_ent_types, manner_ent_types, agent_ent_types, \\\n",
        "    patient_ent_types, state_ent_types, cause_ent_types, loc_ent_types = semantic_role_ent_type\n",
        "\n",
        "    # main pronouns\n",
        "    pronouns = ['i', 'you', 'he', 'she', 'we', 'they']\n",
        "    to_be_s = ['am', 'are', 'is', 'was', 'were']\n",
        "\n",
        "    extra_verb = find_extra_verb(sent, verb_index)\n",
        "    verb_pos = sent_doc[verb_index].tag_\n",
        "    old_verb = verb\n",
        "\n",
        "    q_ext = ''\n",
        "\n",
        "    if agent_exists and patient_exists:\n",
        "\n",
        "      agent_index = agent_indexes[0]\n",
        "      patient_index = patient_indexes[0]\n",
        "\n",
        "      # get auxiliary verbs\n",
        "      aux = get_aux(sent_doc, verb_index)\n",
        "\n",
        "      if aux == 'to':\n",
        "        continue\n",
        "\n",
        "      if verb_pos == 'VBD':\n",
        "        q_ext = 'did'\n",
        "        verb = get_verb_lemma(verb)\n",
        "      elif verb_pos in ['VB', 'VBP', 'VBZ']:\n",
        "        if is_plural(sent, agent_index):\n",
        "          q_ext = 'do'\n",
        "          verb = get_verb_lemma(verb)\n",
        "        else:\n",
        "          q_ext = 'does'\n",
        "          verb = get_verb_lemma(verb)\n",
        "      elif verb_pos == 'VBG' and not any([to_be in verb for to_be in to_be_s]):\n",
        "        continue\n",
        "      elif not verb_pos.startswith('V'):\n",
        "        continue\n",
        "\n",
        "      q_ext = aux if aux != '' else q_ext\n",
        "\n",
        "\n",
        "      # question filtering\n",
        "      ent_threshold = 0\n",
        "      agent_ent_num = find_num_ent(entities, agent_indexes)\n",
        "      patient_ent_num = find_num_ent(entities, patient_indexes)\n",
        "      ent_sum = agent_ent_num + patient_ent_num\n",
        "\n",
        "      max_q_thresh = 500\n",
        "\n",
        "\n",
        "      q_wh = find_wh_question(patient_ent_types, 'patient', all_ents[patient_indexes[0]: patient_indexes[1] + 1], patient)\n",
        "      q_wh = extra_check_person(patient, q_wh)\n",
        "\n",
        "\n",
        "      # fix answer in the right way\n",
        "      if simple_pronoun(patient) and patient_cluster_idx is not None:\n",
        "        patient = get_non_trivial_coref(patient_corefs)\n",
        "\n",
        "      if patient == '':\n",
        "        continue\n",
        "\n",
        "\n",
        "      Y = give_new_ents(patient, patient_indexes, patient_cluster_idx[0], all_ents)\n",
        "\n",
        "      q_wh = find_wh_question(patient_ent_types, 'patient', Y, patient)\n",
        "      q_wh = extra_check_person(patient, q_wh)\n",
        "\n",
        "      root_word = get_sentence_root(patient)\n",
        "      if root_word.pos_ == 'VERB':\n",
        "        q_wh = 'what'\n",
        "\n",
        "      question = get_sent(q_wh, q_ext, agent, verb, state, arg3, arg4, time, loc, '?')\n",
        "\n",
        "\n",
        "      q_detail = {\n",
        "        'bef': get_sent(q_wh, q_ext),\n",
        "        'agent': agent,\n",
        "        'after': get_sent(extra_verb, verb, state, arg3, arg4, time, loc, '?')\n",
        "      }\n",
        "\n",
        "\n",
        "      answer = patient\n",
        "      if answer == 'this' or answer == 'that' or answer == 'when' or answer == 'where':\n",
        "        continue\n",
        "      question, answer = clean_text(question), clean_text(answer)\n",
        "\n",
        "      Qs.append({\n",
        "          'q_wh': q_wh,\n",
        "          'q_ext': q_ext,\n",
        "          'agent': agent,\n",
        "          'verb': verb,\n",
        "          'state': state,\n",
        "          'arg3': arg3,\n",
        "          'arg4': arg4,\n",
        "          'time': time,\n",
        "          'loc': loc,\n",
        "          'answer': answer,\n",
        "          'agent': agent,\n",
        "          'agent_cluster_idx': agent_cluster_idx,\n",
        "      })\n",
        "\n",
        "      question = make_final_text(question)\n",
        "      answer = make_final_text(answer)\n",
        "\n",
        "      QA = get_sent(question, answer)\n",
        "      print(nidx, '-', QA)\n",
        "\n",
        "      agent_list.append([agent, agent_cluster_idx])\n",
        "\n",
        "      Agent_Patient_Qs.append({\n",
        "          'question': question,\n",
        "          'answer': answer,\n",
        "          'agent': agent,\n",
        "          'q_ext': q_ext,\n",
        "          'patient': patient,\n",
        "          'sent': sent,\n",
        "          'old_verb': old_verb,\n",
        "          'verb': verb,\n",
        "          'aux': aux,\n",
        "          'root': root_word.text,\n",
        "          'root_pos': root_word.pos_,\n",
        "          'pt': semantic_role_ent_type,\n",
        "          'ents': entities,\n",
        "          'extra_verb': extra_verb,\n",
        "          'verb_indexes': verb_indexes,\n",
        "          'verb_pos': verb_pos,\n",
        "          'q_wh': q_wh,\n",
        "          'q_ext': q_ext,\n",
        "          'agent_cluster': agent_cluster_idx,\n",
        "          'patient_cluster': patient_cluster_idx,\n",
        "          'agent_in_question': True,\n",
        "          'q_detail': q_detail,\n",
        "          'sent_idx': sent_idx\n",
        "      })\n",
        "\n",
        "      nidx += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xhGIbm0wFVjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_2_list = dict()\n",
        "for Q in Qs:\n",
        "  cluster_id = Q['agent_cluster_idx'][0]\n",
        "  if cluster_id not in idx_2_list.keys():\n",
        "    idx_2_list[cluster_id] = []\n",
        "  idx_2_list[cluster_id].append(Q)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nLIaByG5fqVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qidx = 1\n",
        "for k, v in idx_2_list.items():\n",
        "  for turn_idx, turn in enumerate(v):\n",
        "    agent_cluster_idx = turn['agent_cluster_idx'][0]\n",
        "    if agent_cluster_idx is not None:\n",
        "      pns = idx_2_cluster[agent_cluster_idx]\n",
        "      if turn_idx == 0:\n",
        "        pn = get_pronoun(pns, simple=False)\n",
        "      else:\n",
        "        pn = get_pronoun(pns, simple=True)\n",
        "    else:\n",
        "      pn = turn['agent']\n",
        "\n",
        "    question = get_sent(turn['q_wh'],\n",
        "                        turn['q_ext'],\n",
        "                        pn,\n",
        "                        turn['verb'],\n",
        "                        turn['state'],\n",
        "                        turn['arg3'],\n",
        "                        turn['arg4'],\n",
        "                        # turn['time'],\n",
        "                        turn['loc'],\n",
        "                        '?')\n",
        "    answer = turn['answer']\n",
        "    question, answer = clean_text(question), clean_text(answer)\n",
        "    question = make_final_text(question)\n",
        "    answer = make_final_text(answer)\n",
        "    print(str(qidx) + '-', question, answer)\n",
        "    qidx += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "I-1V3xrlFZ_-",
        "outputId": "8aa1031e-072b-47ee-f412-af0a712f6a6f"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1- what did parsons win For his portrayal? four Primetime Emmy Awards, a Golden Globe Award, a TCA Award, and two Critics' Choice Television Awards\n",
            "2- what has he said? that in his opinion, Sheldon \"couldn't display more facets\" of Asperger's syndrome\n",
            "3- what does the role have? a genius-level IQ of 187\n",
            "4- what did he state In Young Sheldon Season 7 Episode 7? that he is treated like a receptionist at home, despite having an IQ of 187, directly confirming the number\n",
            "5- what does he display? a fundamental lack of social skills, a tenuous understanding of humor (always ending with \"bazinga\"), and difficulty recognizing irony and sarcasm in other people\n",
            "6- what does he employ? irony and sarcasm\n",
            "7- what does he exhibit? highly idiosyncratic behavior and a general lack of humility, empathy, and toleration\n",
            "8- what couldn't he display? more facets \"of Asperger's syndrome\n",
            "9- what did he get? his father fired\n",
            "10- what did he cherish? his maternal grandfather\n",
            "11- what did he call \"Pop-Pop\"? his maternal grandfather\n",
            "12- what does he call \"Meemaw\"? his maternal grandmother\n",
            "13- what'd he have? a' trade' to fall back on\n",
            "14- what does leonard have? an IQ of 173\n",
            "15- what does highly idiosyncratic behavior and a general lack of humility, empathy, and toleration provide? the majority of the humor involving him, which are credited with making him the show's breakout character\n",
            "16- what have Some viewers asserted? that Sheldon's personality is consistent with autism spectrum disorder (or what used to be classified as Asperger's Syndrome)\n",
            "17- what has Co-creator Bill Prady stated? that Sheldon's character was neither conceived nor developed with regard to Asperger's\n",
            "18- what was by a computer programmer personally known to series co-creator Bill Prady inspired? The character of Sheldon Cooper\n",
            "19- what to have The only member of his family encouraged? his work in science\n",
            "20- what did Pop-Pop's loss cause? Sheldon to not like Christmas very much when his Christmas wish to bring Pop-Pop back did not come true\n",
            "21- what does his wish? to bring Pop-Pop back\n",
            "22- who does his maternal grandmother call \"Moon Pie\"? Sheldon Lee Cooper, Ph.D., Sc.D.\n",
            "23- what to have His aunt encouraged? his work in science\n",
            "24- who did the one introduce to non-scientific interests such as comic books and Dungeons & Dragons? Sheldon Lee Cooper, Ph.D., Sc.D.\n",
            "25- what did lorre intend? Johnny Galecki to play the role\n",
            "26- who did lorre say? that when Jim Parsons auditioned for the role, he was \"so startlingly good\" that he was asked to reaudition \"to make sure he hadn't gotten lucky\"\n",
            "27- what did johnny galecki think? he would be \"better suited\" for the character of Leonard\n",
            "28- what did george tell Mr. Hinckley, a store owner,? that George was stealing from the cash register\n",
            "29- what did he disclose? that other coaches were illegally recruiting players to their school, forcing the family to return to Medford\n",
            "1- what did parsons win For his portrayal? four Primetime Emmy Awards, a Golden Globe Award, a TCA Award, and two Critics' Choice Television Awards\n",
            "2- what has he said? that in his opinion, Sheldon \"couldn't display more facets\" of Asperger's syndrome\n",
            "3- what does the role have? a genius-level IQ of 187\n",
            "4- what did he state In Young Sheldon Season 7 Episode 7? that he is treated like a receptionist at home, despite having an IQ of 187, directly confirming the number\n",
            "5- what does he display? a fundamental lack of social skills, a tenuous understanding of humor (always ending with \"bazinga\"), and difficulty recognizing irony and sarcasm in other people\n",
            "6- what does he employ? irony and sarcasm\n",
            "7- what does he exhibit? highly idiosyncratic behavior and a general lack of humility, empathy, and toleration\n",
            "8- what couldn't he display? more facets \"of Asperger's syndrome\n",
            "9- what did he get? his father fired\n",
            "10- what did he cherish? his maternal grandfather\n",
            "11- what did he call \"Pop-Pop\"? his maternal grandfather\n",
            "12- what does he call \"Meemaw\"? his maternal grandmother\n",
            "13- what'd he have? a' trade' to fall back on\n",
            "14- what does leonard have? an IQ of 173\n",
            "15- what does highly idiosyncratic behavior and a general lack of humility, empathy, and toleration provide? the majority of the humor involving him, which are credited with making him the show's breakout character\n",
            "16- what have Some viewers asserted? that Sheldon's personality is consistent with autism spectrum disorder (or what used to be classified as Asperger's Syndrome)\n",
            "17- what has Co-creator Bill Prady stated? that Sheldon's character was neither conceived nor developed with regard to Asperger's\n",
            "18- what was by a computer programmer personally known to series co-creator Bill Prady inspired? The character of Sheldon Cooper\n",
            "19- what to have The only member of his family encouraged? his work in science\n",
            "20- what did Pop-Pop's loss cause? Sheldon to not like Christmas very much when his Christmas wish to bring Pop-Pop back did not come true\n",
            "21- what does his wish? to bring Pop-Pop back\n",
            "22- who does his maternal grandmother call \"Moon Pie\"? Sheldon Lee Cooper, Ph.D., Sc.D.\n",
            "23- what to have His aunt encouraged? his work in science\n",
            "24- who did the one introduce to non-scientific interests such as comic books and Dungeons & Dragons? Sheldon Lee Cooper, Ph.D., Sc.D.\n",
            "25- what did lorre intend? Johnny Galecki to play the role\n",
            "26- who did lorre say? that when Jim Parsons auditioned for the role, he was \"so startlingly good\" that he was asked to reaudition \"to make sure he hadn't gotten lucky\"\n",
            "27- what did johnny galecki think? he would be \"better suited\" for the character of Leonard\n",
            "28- what did george tell Mr. Hinckley, a store owner,? that George was stealing from the cash register\n",
            "29- what did he disclose? that other coaches were illegally recruiting players to their school, forcing the family to return to Medford\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6rGO8E78guaX",
        "H1iMhHYz72yL"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}